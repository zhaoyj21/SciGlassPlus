{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aa1cfb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11c8ffb30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import joblib\n",
    "import random\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)           # Python random seed\n",
    "np.random.seed(SEED)        # NumPy random seed\n",
    "torch.manual_seed(SEED)     # PyTorch CPU random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3177a062",
   "metadata": {},
   "source": [
    "# 1. load database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0e7fb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file = pd.ExcelFile('SciGlass_Plus_properties.xlsx')\n",
    "df = excel_file.parse('Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b334b89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_list = ['SiO2','P2O5','ZrO2','Na2O','Al2O3','Fe2O3','CaO','MgO','K2O','MnO','GeO2','Li2O','Ta2O5','ZnO','SrO','CdO','SnO2','B2O3','La2O3','Ga2O3','Y2O3','TiO2','Nb2O5','PbO','HfO2','WO3','Sb2O3','Bi2O3','BaO','Cr2O3','Cu2O','BeO','CuO','Nd2O3','CeO2','Cs2O','As2O3','Rb2O','MoO3','FeO','Mn2O3','ThO2','Ag2O','TeO2','Tl2O','CoO','In2O3','Sc2O3','NiO','V2O5','As2O5','MnO2','Sm2O3','Gd2O3','Tb2O3','Dy2O3','Ho2O3','Er2O3','Yb2O3','Co3O4','Fe3O4','SnO','Mn3O4','Ce2O3','Pr2O3','CrO3','VO6','TeO3','UO2','Sb2O5','Pr6O11','VO2','Co2O3','Ti2O3','UO3','Eu2O3','Mo2O3','Ni2O3','MoO','PrO2','TbO2','V2O3','SeO2','Lu2O3','GeO','SbO2','U3O8','Mn2O7','HgO','Tm2O3','Nb2O3','PbO2','Tl2O3','Pb3O4','SiO','Sn2O3','Ta2O3','RuO2','Tb4O7','Tb3O7','SeO3','Cr3O4','PdO','MoO2','Rh2O3','TiO','RhO2','ErO2','YbO2','Bi2O5','EuO','Pu2O3','CeO','CrO','BaO2','Au2O3','ReO3','Re2O7']\n",
    "\n",
    "filtered_df = df[(df[\"YoungModulus\"] > 0) & (df[\"AnnealTemperature\"] > 0) & (df[comp_list].sum(axis=1) > 0)]\n",
    "filtered_df_unique = filtered_df.drop_duplicates(subset=comp_list + [\"AnnealTemperature\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27664cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = filtered_df_unique[\"YoungModulus\"].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f502202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(747, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9193c0de",
   "metadata": {},
   "source": [
    "# 2. pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accecca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_bad_compositions(df, comp_list, tol=1.0, normalize=False):\n",
    "    \n",
    "    df = df.copy()\n",
    "    df[\"CompSum\"] = df[comp_list].sum(axis=1)\n",
    "    \n",
    "    lower, upper = 100 - tol, 100 + tol\n",
    "    \n",
    "    df_clean = df[(df[\"CompSum\"] >= lower) & (df[\"CompSum\"] <= upper)].copy()\n",
    "    removed = len(df) - len(df_clean)\n",
    "    \n",
    "    if normalize:\n",
    "        df_clean[comp_list] = df_clean[comp_list].div(df_clean[\"CompSum\"], axis=0) * 100\n",
    "    \n",
    "    df_clean = df_clean.drop(columns=[\"CompSum\"])\n",
    "    return df_clean\n",
    "\n",
    "def clean_near_duplicates(df, feature_cols, target_col=\"YoungModulus\", \n",
    "                          x_tol=0.05, y_tol=0.3):\n",
    "\n",
    "    # -------- Step 1. 去掉完全重复的 X --------\n",
    "    #df_unique = df.groupby(feature_cols, as_index=False).agg({target_col: \"mean\"})\n",
    "    df_unique = df\n",
    "\n",
    "    X = df_unique[feature_cols].fillna(0).values\n",
    "    y = df_unique[target_col].fillna(0).values\n",
    "    \n",
    "    dist_matrix = pairwise_distances(X, metric=\"euclidean\")\n",
    "    np.fill_diagonal(dist_matrix, np.inf)\n",
    "    \n",
    "    bad_idx = set()\n",
    "    for i in range(len(X)):\n",
    "\n",
    "        close_idx = np.where(dist_matrix[i] < x_tol)[0]\n",
    "        for j in close_idx:\n",
    "            if abs(y[i] - y[j]) > y_tol * np.mean([y[i], y[j]]):\n",
    "                bad_idx.add(i)\n",
    "                bad_idx.add(j)\n",
    "    \n",
    "    df_clean = df_unique.drop(df_unique.index[list(bad_idx)]).reset_index(drop=True)\n",
    "    return df_clean\n",
    "\n",
    "filtered_df_clean0 = filter_bad_compositions(\n",
    "    filtered_df_unique,\n",
    "    comp_list=comp_list,\n",
    "    tol=1.0,         \n",
    "    normalize=False\n",
    ")\n",
    "\n",
    "filtered_df_clean = clean_near_duplicates(\n",
    "    filtered_df_clean0,\n",
    "    feature_cols = comp_list + [\"AnnealTemperature\"], \n",
    "    target_col = \"YoungModulus\",\n",
    "    x_tol = 0.5,\n",
    "    y_tol = 0.2\n",
    ")\n",
    "\n",
    "X1 = filtered_df_clean[comp_list].fillna(0).values/ 100.0\n",
    "y = filtered_df_clean[\"YoungModulus\"].values.reshape(-1, 1)\n",
    "\n",
    "X2 = filtered_df_clean[comp_list + [\"AnnealTemperature\"]].fillna(0).copy()\n",
    "X2.loc[:, comp_list] = X2[comp_list] / 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26ae02a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(563, 728)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff7ffec",
   "metadata": {},
   "source": [
    "# 3. train, valid, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72a8749b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fk/ztcjdp395590k9d3hts6xcbw0000gn/T/ipykernel_1751/638799307.py:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[[-2.36014983]\n",
      " [-2.36014983]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-1.60926063]\n",
      " [-1.60926063]\n",
      " [-1.60926063]\n",
      " [-1.60926063]\n",
      " [-1.60926063]\n",
      " [-1.60926063]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [ 3.31323525]\n",
      " [ 3.31323525]\n",
      " [ 3.31323525]\n",
      " [ 2.92110423]\n",
      " [ 3.89726019]\n",
      " [-0.27434649]\n",
      " [-0.27434649]\n",
      " [-0.27434649]\n",
      " [-0.27434649]\n",
      " [-0.77493929]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.85837143]\n",
      " [-0.9668332 ]\n",
      " [-0.9668332 ]\n",
      " [-1.06695176]\n",
      " [-0.85837143]\n",
      " [-0.85837143]\n",
      " [-0.85837143]\n",
      " [-0.85837143]\n",
      " [-0.85837143]\n",
      " [-0.60807503]\n",
      " [-0.60807503]\n",
      " [-0.60807503]\n",
      " [-0.60807503]\n",
      " [-0.60807503]\n",
      " [-1.60926063]\n",
      " [-1.60926063]\n",
      " [-1.60926063]\n",
      " [-1.60926063]\n",
      " [-1.60926063]\n",
      " [ 0.47654271]\n",
      " [ 0.47654271]\n",
      " [ 0.47654271]\n",
      " [ 0.47654271]\n",
      " [ 0.47654271]\n",
      " [ 0.97338107]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-1.44239636]\n",
      " [-1.32559137]\n",
      " [-1.03357891]\n",
      " [-1.04192212]\n",
      " [-0.92511713]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77619077]\n",
      " [-0.77619077]\n",
      " [-0.77619077]\n",
      " [-0.77619077]\n",
      " [-0.77619077]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [ 2.14518538]\n",
      " [ 1.14399978]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [ 1.31086405]\n",
      " [ 1.31086405]\n",
      " [ 1.31086405]\n",
      " [ 1.31086405]\n",
      " [ 1.31086405]\n",
      " [ 1.31086405]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [ 0.47654271]\n",
      " [ 0.47654271]\n",
      " [ 0.47654271]\n",
      " [ 0.47654271]\n",
      " [ 0.47654271]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-1.19335144]\n",
      " [-1.19335144]\n",
      " [-1.19335144]\n",
      " [-1.19335144]\n",
      " [-0.19091436]\n",
      " [-0.19091436]\n",
      " [-0.19091436]\n",
      " [-0.19091436]\n",
      " [-0.19091436]\n",
      " [-0.19091436]\n",
      " [-0.19091436]\n",
      " [-0.19091436]\n",
      " [-0.19091436]\n",
      " [-0.19091436]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [ 0.47654271]\n",
      " [ 0.47654271]\n",
      " [ 1.72802472]\n",
      " [ 1.72802472]\n",
      " [ 1.72802472]\n",
      " [ 1.72802472]\n",
      " [ 1.72802472]\n",
      " [ 1.72802472]\n",
      " [ 1.72802472]\n",
      " [ 1.72802472]\n",
      " [ 1.72802472]\n",
      " [ 1.72802472]\n",
      " [ 1.72802472]\n",
      " [ 1.72802472]\n",
      " [ 2.14518538]\n",
      " [ 2.14518538]\n",
      " [ 2.14518538]\n",
      " [ 2.14518538]\n",
      " [ 2.14518538]\n",
      " [ 2.14518538]\n",
      " [ 2.14518538]\n",
      " [ 2.14518538]\n",
      " [ 2.14518538]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [ 1.06056765]\n",
      " [ 1.06056765]\n",
      " [ 1.10228371]\n",
      " [ 0.66843662]\n",
      " [ 0.68512305]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 2.56234605]\n",
      " [ 2.56234605]\n",
      " [ 1.72802472]\n",
      " [ 1.72802472]\n",
      " [ 1.72802472]\n",
      " [ 1.72802472]\n",
      " [ 1.72802472]\n",
      " [ 1.72802472]\n",
      " [ 1.72802472]\n",
      " [ 1.72802472]\n",
      " [ 1.72802472]\n",
      " [ 1.72802472]\n",
      " [ 1.72802472]\n",
      " [ 1.72802472]\n",
      " [ 1.72802472]\n",
      " [ 1.72802472]\n",
      " [ 1.72802472]\n",
      " [ 1.72802472]\n",
      " [ 1.72802472]\n",
      " [ 1.72802472]\n",
      " [ 1.72802472]\n",
      " [ 1.72802472]\n",
      " [ 3.43713197]\n",
      " [ 3.22020842]\n",
      " [ 3.37872948]\n",
      " [ 3.47050483]\n",
      " [ 3.46216161]\n",
      " [ 0.49531494]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-1.19209996]\n",
      " [-1.19209996]\n",
      " [-1.19209996]\n",
      " [-1.19209996]\n",
      " [-1.19209996]\n",
      " [-1.19209996]\n",
      " [ 0.47654271]\n",
      " [ 0.47654271]\n",
      " [ 0.47654271]\n",
      " [ 0.47654271]\n",
      " [ 0.47654271]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [ 3.13802777]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 1.31086405]\n",
      " [ 1.31086405]\n",
      " [ 1.31086405]\n",
      " [ 1.31086405]\n",
      " [ 1.31086405]\n",
      " [ 1.31086405]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [ 1.22743191]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.60807503]\n",
      " [-0.60807503]\n",
      " [-0.60807503]\n",
      " [-0.60807503]\n",
      " [-0.60807503]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [-0.60807503]\n",
      " [-0.60807503]\n",
      " [-0.60807503]\n",
      " [-0.60807503]\n",
      " [-1.19209996]\n",
      " [-1.19209996]\n",
      " [-1.19209996]\n",
      " [-1.19209996]\n",
      " [-1.19209996]\n",
      " [-1.19209996]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 1.31086405]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [ 0.89370338]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-1.19209996]\n",
      " [-1.19209996]\n",
      " [-1.19209996]\n",
      " [-1.19209996]\n",
      " [-1.19209996]\n",
      " [ 0.47654271]\n",
      " [ 0.47654271]\n",
      " [ 0.47654271]\n",
      " [ 0.47654271]\n",
      " [ 0.47654271]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-0.35777862]\n",
      " [-1.19209996]\n",
      " [-1.19209996]\n",
      " [-1.19209996]\n",
      " [-1.19209996]\n",
      " [-1.19209996]\n",
      " [-1.19209996]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.47654271]\n",
      " [ 0.47654271]\n",
      " [ 0.47654271]\n",
      " [ 0.47654271]\n",
      " [ 0.47654271]\n",
      " [ 0.47654271]\n",
      " [ 0.47654271]\n",
      " [ 0.47654271]\n",
      " [ 0.47654271]\n",
      " [ 0.47654271]\n",
      " [ 0.47654271]\n",
      " [ 3.73039592]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [ 0.05938204]\n",
      " [-0.02405009]\n",
      " [-0.02405009]\n",
      " [-0.02405009]\n",
      " [-0.02405009]\n",
      " [-0.02405009]\n",
      " [-1.38524535]\n",
      " [-1.38524535]\n",
      " [-1.38524535]\n",
      " [-1.38524535]\n",
      " [-1.38524535]\n",
      " [-1.38524535]\n",
      " [-1.38524535]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.94180356]\n",
      " [-0.10748222]\n",
      " [-0.10748222]\n",
      " [-0.10748222]\n",
      " [-0.10748222]\n",
      " [-0.10748222]\n",
      " [-0.10748222]\n",
      " [-0.77619077]\n",
      " [-0.77619077]\n",
      " [-0.77619077]\n",
      " [-0.77619077]\n",
      " [-0.77619077]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]\n",
      " [-0.77493929]]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  X2.loc[:, \"AnnealTemperature\"] = scaler_temp.transform(X2[[\"AnnealTemperature\"]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Run 1/10 =====\n",
      "Epoch 50, Validation Loss: 799.1247\n",
      "Epoch 100, Validation Loss: 794.1595\n",
      "Epoch 150, Validation Loss: 782.9171\n",
      "Epoch 200, Validation Loss: 766.5204\n",
      "Epoch 250, Validation Loss: 747.4532\n",
      "Epoch 300, Validation Loss: 726.6767\n",
      "Epoch 350, Validation Loss: 706.3056\n",
      "Epoch 400, Validation Loss: 690.4323\n",
      "Epoch 450, Validation Loss: 677.5424\n",
      "Epoch 500, Validation Loss: 667.0900\n",
      "Epoch 550, Validation Loss: 656.1079\n",
      "Epoch 600, Validation Loss: 640.6203\n",
      "Epoch 650, Validation Loss: 622.9776\n",
      "Epoch 700, Validation Loss: 604.2815\n",
      "Epoch 750, Validation Loss: 580.6821\n",
      "Epoch 800, Validation Loss: 554.2822\n",
      "Epoch 850, Validation Loss: 532.0562\n",
      "Epoch 900, Validation Loss: 509.5232\n",
      "Epoch 950, Validation Loss: 485.2414\n",
      "Epoch 1000, Validation Loss: 465.2838\n",
      "Epoch 1050, Validation Loss: 451.0365\n",
      "Epoch 1100, Validation Loss: 445.5175\n",
      "Epoch 1150, Validation Loss: 439.8187\n",
      "Epoch 1200, Validation Loss: 430.5922\n",
      "Epoch 1250, Validation Loss: 420.5833\n",
      "Epoch 1300, Validation Loss: 414.8788\n",
      "Epoch 1350, Validation Loss: 415.5917\n",
      "Epoch 1400, Validation Loss: 430.9805\n",
      "Epoch 1450, Validation Loss: 458.1340\n",
      "Epoch 1500, Validation Loss: 479.0440\n",
      "\n",
      "===== Run 2/10 =====\n",
      "Epoch 50, Validation Loss: 799.1010\n",
      "Epoch 100, Validation Loss: 793.0614\n",
      "Epoch 150, Validation Loss: 780.5666\n",
      "Epoch 200, Validation Loss: 762.8474\n",
      "Epoch 250, Validation Loss: 743.2960\n",
      "Epoch 300, Validation Loss: 724.3535\n",
      "Epoch 350, Validation Loss: 707.7160\n",
      "Epoch 400, Validation Loss: 693.1181\n",
      "Epoch 450, Validation Loss: 681.7376\n",
      "Epoch 500, Validation Loss: 672.9264\n",
      "Epoch 550, Validation Loss: 664.9088\n",
      "Epoch 600, Validation Loss: 656.6668\n",
      "Epoch 650, Validation Loss: 648.9810\n",
      "Epoch 700, Validation Loss: 641.4498\n",
      "Epoch 750, Validation Loss: 633.8829\n",
      "Epoch 800, Validation Loss: 625.1278\n",
      "Epoch 850, Validation Loss: 610.1971\n",
      "Epoch 900, Validation Loss: 585.1075\n",
      "Epoch 950, Validation Loss: 553.0386\n",
      "Epoch 1000, Validation Loss: 518.2968\n",
      "Epoch 1050, Validation Loss: 488.1827\n",
      "Epoch 1100, Validation Loss: 466.4804\n",
      "Epoch 1150, Validation Loss: 454.7646\n",
      "Epoch 1200, Validation Loss: 449.0897\n",
      "Epoch 1250, Validation Loss: 454.6752\n",
      "Epoch 1300, Validation Loss: 458.7509\n",
      "Epoch 1350, Validation Loss: 457.1859\n",
      "Epoch 1400, Validation Loss: 459.5827\n",
      "Epoch 1450, Validation Loss: 451.9090\n",
      "Epoch 1500, Validation Loss: 447.1078\n",
      "\n",
      "===== Run 3/10 =====\n",
      "Epoch 50, Validation Loss: 799.9149\n",
      "Epoch 100, Validation Loss: 793.6923\n",
      "Epoch 150, Validation Loss: 779.5179\n",
      "Epoch 200, Validation Loss: 759.4005\n",
      "Epoch 250, Validation Loss: 736.4784\n",
      "Epoch 300, Validation Loss: 713.5622\n",
      "Epoch 350, Validation Loss: 693.8925\n",
      "Epoch 400, Validation Loss: 677.2773\n",
      "Epoch 450, Validation Loss: 663.9105\n",
      "Epoch 500, Validation Loss: 652.5795\n",
      "Epoch 550, Validation Loss: 642.0120\n",
      "Epoch 600, Validation Loss: 632.6817\n",
      "Epoch 650, Validation Loss: 623.4044\n",
      "Epoch 700, Validation Loss: 610.5417\n",
      "Epoch 750, Validation Loss: 591.3184\n",
      "Epoch 800, Validation Loss: 566.0933\n",
      "Epoch 850, Validation Loss: 531.7664\n",
      "Epoch 900, Validation Loss: 501.7916\n",
      "Epoch 950, Validation Loss: 477.4695\n",
      "Epoch 1000, Validation Loss: 461.3827\n",
      "Epoch 1050, Validation Loss: 445.0155\n",
      "Epoch 1100, Validation Loss: 435.6288\n",
      "Epoch 1150, Validation Loss: 431.4513\n",
      "Epoch 1200, Validation Loss: 428.1162\n",
      "Epoch 1250, Validation Loss: 425.9800\n",
      "Epoch 1300, Validation Loss: 423.1777\n",
      "Epoch 1350, Validation Loss: 420.4120\n",
      "Epoch 1400, Validation Loss: 420.4647\n",
      "Epoch 1450, Validation Loss: 421.5442\n",
      "Epoch 1500, Validation Loss: 426.8902\n",
      "\n",
      "===== Run 4/10 =====\n",
      "Epoch 50, Validation Loss: 796.1384\n",
      "Epoch 100, Validation Loss: 791.2056\n",
      "Epoch 150, Validation Loss: 778.9181\n",
      "Epoch 200, Validation Loss: 760.7731\n",
      "Epoch 250, Validation Loss: 739.5404\n",
      "Epoch 300, Validation Loss: 718.4510\n",
      "Epoch 350, Validation Loss: 699.5009\n",
      "Epoch 400, Validation Loss: 683.3029\n",
      "Epoch 450, Validation Loss: 666.8116\n",
      "Epoch 500, Validation Loss: 649.5792\n",
      "Epoch 550, Validation Loss: 629.8375\n",
      "Epoch 600, Validation Loss: 607.3187\n",
      "Epoch 650, Validation Loss: 584.0713\n",
      "Epoch 700, Validation Loss: 563.6393\n",
      "Epoch 750, Validation Loss: 538.9660\n",
      "Epoch 800, Validation Loss: 512.7831\n",
      "Epoch 850, Validation Loss: 489.7761\n",
      "Epoch 900, Validation Loss: 474.5870\n",
      "Epoch 950, Validation Loss: 465.6788\n",
      "Epoch 1000, Validation Loss: 464.2851\n",
      "Epoch 1050, Validation Loss: 446.3379\n",
      "Epoch 1100, Validation Loss: 422.7192\n",
      "Epoch 1150, Validation Loss: 397.5389\n",
      "Epoch 1200, Validation Loss: 380.6457\n",
      "Epoch 1250, Validation Loss: 374.7268\n",
      "Epoch 1300, Validation Loss: 373.2124\n",
      "Epoch 1350, Validation Loss: 374.5278\n",
      "Epoch 1400, Validation Loss: 376.6103\n",
      "Epoch 1450, Validation Loss: 376.4355\n",
      "Epoch 1500, Validation Loss: 374.1773\n",
      "\n",
      "===== Run 5/10 =====\n",
      "Epoch 50, Validation Loss: 795.9883\n",
      "Epoch 100, Validation Loss: 791.9009\n",
      "Epoch 150, Validation Loss: 780.6510\n",
      "Epoch 200, Validation Loss: 763.8238\n",
      "Epoch 250, Validation Loss: 744.8173\n",
      "Epoch 300, Validation Loss: 726.1233\n",
      "Epoch 350, Validation Loss: 709.3928\n",
      "Epoch 400, Validation Loss: 695.1608\n",
      "Epoch 450, Validation Loss: 683.8377\n",
      "Epoch 500, Validation Loss: 674.0398\n",
      "Epoch 550, Validation Loss: 665.2734\n",
      "Epoch 600, Validation Loss: 658.1893\n",
      "Epoch 650, Validation Loss: 650.2686\n",
      "Epoch 700, Validation Loss: 642.6187\n",
      "Epoch 750, Validation Loss: 634.3364\n",
      "Epoch 800, Validation Loss: 622.1616\n",
      "Epoch 850, Validation Loss: 608.1496\n",
      "Epoch 900, Validation Loss: 591.1766\n",
      "Epoch 950, Validation Loss: 570.9803\n",
      "Epoch 1000, Validation Loss: 546.1737\n",
      "Epoch 1050, Validation Loss: 515.9953\n",
      "Epoch 1100, Validation Loss: 487.6853\n",
      "Epoch 1150, Validation Loss: 470.1405\n",
      "Epoch 1200, Validation Loss: 464.9182\n",
      "Epoch 1250, Validation Loss: 469.9840\n",
      "Epoch 1300, Validation Loss: 477.6124\n",
      "Epoch 1350, Validation Loss: 482.4794\n",
      "Epoch 1400, Validation Loss: 481.1889\n",
      "Epoch 1450, Validation Loss: 472.6481\n",
      "Epoch 1500, Validation Loss: 463.5291\n",
      "\n",
      "===== Run 6/10 =====\n",
      "Epoch 50, Validation Loss: 801.5242\n",
      "Epoch 100, Validation Loss: 794.0948\n",
      "Epoch 150, Validation Loss: 780.4861\n",
      "Epoch 200, Validation Loss: 761.9526\n",
      "Epoch 250, Validation Loss: 741.4622\n",
      "Epoch 300, Validation Loss: 721.4005\n",
      "Epoch 350, Validation Loss: 703.9189\n",
      "Epoch 400, Validation Loss: 687.5509\n",
      "Epoch 450, Validation Loss: 674.6703\n",
      "Epoch 500, Validation Loss: 663.7376\n",
      "Epoch 550, Validation Loss: 653.7032\n",
      "Epoch 600, Validation Loss: 644.8104\n",
      "Epoch 650, Validation Loss: 636.5320\n",
      "Epoch 700, Validation Loss: 628.8217\n",
      "Epoch 750, Validation Loss: 620.8748\n",
      "Epoch 800, Validation Loss: 610.1768\n",
      "Epoch 850, Validation Loss: 594.6935\n",
      "Epoch 900, Validation Loss: 571.6211\n",
      "Epoch 950, Validation Loss: 548.2043\n",
      "Epoch 1000, Validation Loss: 528.0407\n",
      "Epoch 1050, Validation Loss: 511.8982\n",
      "Epoch 1100, Validation Loss: 493.9678\n",
      "Epoch 1150, Validation Loss: 460.6844\n",
      "Epoch 1200, Validation Loss: 420.2498\n",
      "Epoch 1250, Validation Loss: 381.7704\n",
      "Epoch 1300, Validation Loss: 353.9974\n",
      "Epoch 1350, Validation Loss: 332.2170\n",
      "Epoch 1400, Validation Loss: 315.0880\n",
      "Epoch 1450, Validation Loss: 303.7054\n",
      "Epoch 1500, Validation Loss: 297.2799\n",
      "\n",
      "===== Run 7/10 =====\n",
      "Epoch 50, Validation Loss: 798.2824\n",
      "Epoch 100, Validation Loss: 792.2739\n",
      "Epoch 150, Validation Loss: 779.2335\n",
      "Epoch 200, Validation Loss: 760.5949\n",
      "Epoch 250, Validation Loss: 739.4099\n",
      "Epoch 300, Validation Loss: 718.2797\n",
      "Epoch 350, Validation Loss: 698.7620\n",
      "Epoch 400, Validation Loss: 682.2003\n",
      "Epoch 450, Validation Loss: 666.9500\n",
      "Epoch 500, Validation Loss: 654.5979\n",
      "Epoch 550, Validation Loss: 641.5968\n",
      "Epoch 600, Validation Loss: 626.9173\n",
      "Epoch 650, Validation Loss: 610.8143\n",
      "Epoch 700, Validation Loss: 592.1989\n",
      "Epoch 750, Validation Loss: 571.0877\n",
      "Epoch 800, Validation Loss: 548.1997\n",
      "Epoch 850, Validation Loss: 519.6022\n",
      "Epoch 900, Validation Loss: 489.9616\n",
      "Epoch 950, Validation Loss: 467.4503\n",
      "Epoch 1000, Validation Loss: 456.6459\n",
      "Epoch 1050, Validation Loss: 457.2458\n",
      "Epoch 1100, Validation Loss: 455.9445\n",
      "Epoch 1150, Validation Loss: 450.3895\n",
      "Epoch 1200, Validation Loss: 442.1148\n",
      "Epoch 1250, Validation Loss: 431.7101\n",
      "Epoch 1300, Validation Loss: 428.4982\n",
      "Epoch 1350, Validation Loss: 431.7820\n",
      "Epoch 1400, Validation Loss: 434.4070\n",
      "Epoch 1450, Validation Loss: 439.6564\n",
      "Epoch 1500, Validation Loss: 445.2388\n",
      "\n",
      "===== Run 8/10 =====\n",
      "Epoch 50, Validation Loss: 793.1221\n",
      "Epoch 100, Validation Loss: 790.6537\n",
      "Epoch 150, Validation Loss: 776.1849\n",
      "Epoch 200, Validation Loss: 753.9234\n",
      "Epoch 250, Validation Loss: 728.2450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300, Validation Loss: 703.0779\n",
      "Epoch 350, Validation Loss: 681.2695\n",
      "Epoch 400, Validation Loss: 664.6148\n",
      "Epoch 450, Validation Loss: 650.8715\n",
      "Epoch 500, Validation Loss: 639.3171\n",
      "Epoch 550, Validation Loss: 629.0811\n",
      "Epoch 600, Validation Loss: 615.1478\n",
      "Epoch 650, Validation Loss: 600.7480\n",
      "Epoch 700, Validation Loss: 585.5789\n",
      "Epoch 750, Validation Loss: 569.5879\n",
      "Epoch 800, Validation Loss: 551.1318\n",
      "Epoch 850, Validation Loss: 532.4848\n",
      "Epoch 900, Validation Loss: 511.8873\n",
      "Epoch 950, Validation Loss: 494.9460\n",
      "Epoch 1000, Validation Loss: 481.1874\n",
      "Epoch 1050, Validation Loss: 466.7105\n",
      "Epoch 1100, Validation Loss: 452.2461\n",
      "Epoch 1150, Validation Loss: 437.9510\n",
      "Epoch 1200, Validation Loss: 425.9205\n",
      "Epoch 1250, Validation Loss: 416.4245\n",
      "Epoch 1300, Validation Loss: 403.3960\n",
      "Epoch 1350, Validation Loss: 390.2080\n",
      "Epoch 1400, Validation Loss: 374.9007\n",
      "Epoch 1450, Validation Loss: 360.9898\n",
      "Epoch 1500, Validation Loss: 346.0764\n",
      "\n",
      "===== Run 9/10 =====\n",
      "Epoch 50, Validation Loss: 800.2228\n",
      "Epoch 100, Validation Loss: 793.2353\n",
      "Epoch 150, Validation Loss: 780.1591\n",
      "Epoch 200, Validation Loss: 761.8996\n",
      "Epoch 250, Validation Loss: 742.3414\n",
      "Epoch 300, Validation Loss: 723.1529\n",
      "Epoch 350, Validation Loss: 705.0093\n",
      "Epoch 400, Validation Loss: 689.6942\n",
      "Epoch 450, Validation Loss: 675.4162\n",
      "Epoch 500, Validation Loss: 661.2672\n",
      "Epoch 550, Validation Loss: 646.4121\n",
      "Epoch 600, Validation Loss: 627.7715\n",
      "Epoch 650, Validation Loss: 608.4971\n",
      "Epoch 700, Validation Loss: 589.0683\n",
      "Epoch 750, Validation Loss: 566.3834\n",
      "Epoch 800, Validation Loss: 541.1764\n",
      "Epoch 850, Validation Loss: 508.3940\n",
      "Epoch 900, Validation Loss: 480.6003\n",
      "Epoch 950, Validation Loss: 455.4548\n",
      "Epoch 1000, Validation Loss: 440.7524\n",
      "Epoch 1050, Validation Loss: 433.4214\n",
      "Epoch 1100, Validation Loss: 431.7576\n",
      "Epoch 1150, Validation Loss: 427.9768\n",
      "Epoch 1200, Validation Loss: 421.4058\n",
      "Epoch 1250, Validation Loss: 415.8257\n",
      "Epoch 1300, Validation Loss: 408.1800\n",
      "Epoch 1350, Validation Loss: 414.9953\n",
      "Epoch 1400, Validation Loss: 428.6888\n",
      "Epoch 1450, Validation Loss: 442.7594\n",
      "Epoch 1500, Validation Loss: 453.9896\n",
      "\n",
      "===== Run 10/10 =====\n",
      "Epoch 50, Validation Loss: 798.2864\n",
      "Epoch 100, Validation Loss: 792.5827\n",
      "Epoch 150, Validation Loss: 780.1205\n",
      "Epoch 200, Validation Loss: 762.3245\n",
      "Epoch 250, Validation Loss: 742.6943\n",
      "Epoch 300, Validation Loss: 723.8496\n",
      "Epoch 350, Validation Loss: 706.6863\n",
      "Epoch 400, Validation Loss: 692.8560\n",
      "Epoch 450, Validation Loss: 682.2289\n",
      "Epoch 500, Validation Loss: 673.7034\n",
      "Epoch 550, Validation Loss: 665.1111\n",
      "Epoch 600, Validation Loss: 658.5963\n",
      "Epoch 650, Validation Loss: 651.9009\n",
      "Epoch 700, Validation Loss: 642.9677\n",
      "Epoch 750, Validation Loss: 632.3338\n",
      "Epoch 800, Validation Loss: 619.5222\n",
      "Epoch 850, Validation Loss: 603.1674\n",
      "Epoch 900, Validation Loss: 582.8509\n",
      "Epoch 950, Validation Loss: 559.6770\n",
      "Epoch 1000, Validation Loss: 532.7319\n",
      "Epoch 1050, Validation Loss: 503.8673\n",
      "Epoch 1100, Validation Loss: 476.0042\n",
      "Epoch 1150, Validation Loss: 458.9631\n",
      "Epoch 1200, Validation Loss: 453.8593\n",
      "Epoch 1250, Validation Loss: 452.0005\n",
      "Epoch 1300, Validation Loss: 452.7244\n",
      "Epoch 1350, Validation Loss: 452.9323\n",
      "Epoch 1400, Validation Loss: 453.0182\n",
      "Epoch 1450, Validation Loss: 451.2011\n",
      "Epoch 1500, Validation Loss: 452.5328\n",
      "\n",
      "model1: 平均 RMSE = 16.119, 各次 RMSE = [16.79164, 17.625734, 16.161089, 16.525656, 17.845488, 15.613222, 15.644742, 15.635151, 16.883497, 18.11564]\n",
      "\n",
      "===== Run 1/10 =====\n",
      "Epoch 50, Validation Loss: 746.1473\n",
      "Epoch 100, Validation Loss: 738.1740\n",
      "Epoch 150, Validation Loss: 724.4356\n",
      "Epoch 200, Validation Loss: 706.2865\n",
      "Epoch 250, Validation Loss: 684.1917\n",
      "Epoch 300, Validation Loss: 659.4489\n",
      "Epoch 350, Validation Loss: 632.4464\n",
      "Epoch 400, Validation Loss: 593.3322\n",
      "Epoch 450, Validation Loss: 544.8457\n",
      "Epoch 500, Validation Loss: 501.9029\n",
      "Epoch 550, Validation Loss: 451.7903\n",
      "Epoch 600, Validation Loss: 397.6177\n",
      "Epoch 650, Validation Loss: 350.9897\n",
      "Epoch 700, Validation Loss: 309.7883\n",
      "Epoch 750, Validation Loss: 278.9114\n",
      "Epoch 800, Validation Loss: 263.1337\n",
      "Epoch 850, Validation Loss: 254.9044\n",
      "Epoch 900, Validation Loss: 251.4549\n",
      "Epoch 950, Validation Loss: 248.0797\n",
      "Epoch 1000, Validation Loss: 248.3487\n",
      "Epoch 1050, Validation Loss: 252.3175\n",
      "Epoch 1100, Validation Loss: 255.6373\n",
      "Epoch 1150, Validation Loss: 257.4091\n",
      "Epoch 1200, Validation Loss: 259.0230\n",
      "Epoch 1250, Validation Loss: 259.5753\n",
      "Epoch 1300, Validation Loss: 257.3292\n",
      "Epoch 1350, Validation Loss: 252.9363\n",
      "Epoch 1400, Validation Loss: 251.7706\n",
      "Epoch 1450, Validation Loss: 253.2366\n",
      "Epoch 1500, Validation Loss: 251.5121\n",
      "\n",
      "===== Run 2/10 =====\n",
      "Epoch 50, Validation Loss: 745.0473\n",
      "Epoch 100, Validation Loss: 733.4478\n",
      "Epoch 150, Validation Loss: 715.4398\n",
      "Epoch 200, Validation Loss: 693.0959\n",
      "Epoch 250, Validation Loss: 663.4888\n",
      "Epoch 300, Validation Loss: 629.9278\n",
      "Epoch 350, Validation Loss: 591.0992\n",
      "Epoch 400, Validation Loss: 550.3814\n",
      "Epoch 450, Validation Loss: 516.1827\n",
      "Epoch 500, Validation Loss: 482.5673\n",
      "Epoch 550, Validation Loss: 439.2144\n",
      "Epoch 600, Validation Loss: 391.2770\n",
      "Epoch 650, Validation Loss: 358.6878\n",
      "Epoch 700, Validation Loss: 345.4369\n",
      "Epoch 750, Validation Loss: 312.9944\n",
      "Epoch 800, Validation Loss: 294.7843\n",
      "Epoch 850, Validation Loss: 288.6970\n",
      "Epoch 900, Validation Loss: 291.0670\n",
      "Epoch 950, Validation Loss: 291.6831\n",
      "Epoch 1000, Validation Loss: 291.6582\n",
      "Epoch 1050, Validation Loss: 290.7293\n",
      "Epoch 1100, Validation Loss: 289.1526\n",
      "Epoch 1150, Validation Loss: 288.2710\n",
      "Epoch 1200, Validation Loss: 285.3917\n",
      "Epoch 1250, Validation Loss: 282.3940\n",
      "Epoch 1300, Validation Loss: 279.2740\n",
      "Epoch 1350, Validation Loss: 275.8400\n",
      "Epoch 1400, Validation Loss: 272.6316\n",
      "Epoch 1450, Validation Loss: 269.3042\n",
      "Epoch 1500, Validation Loss: 265.6391\n",
      "\n",
      "===== Run 3/10 =====\n",
      "Epoch 50, Validation Loss: 744.5308\n",
      "Epoch 100, Validation Loss: 733.1009\n",
      "Epoch 150, Validation Loss: 716.3158\n",
      "Epoch 200, Validation Loss: 697.9263\n",
      "Epoch 250, Validation Loss: 674.7958\n",
      "Epoch 300, Validation Loss: 641.9626\n",
      "Epoch 350, Validation Loss: 607.2797\n",
      "Epoch 400, Validation Loss: 571.4698\n",
      "Epoch 450, Validation Loss: 534.2647\n",
      "Epoch 500, Validation Loss: 487.8209\n",
      "Epoch 550, Validation Loss: 423.4816\n",
      "Epoch 600, Validation Loss: 341.8519\n",
      "Epoch 650, Validation Loss: 282.1234\n",
      "Epoch 700, Validation Loss: 248.4379\n",
      "Epoch 750, Validation Loss: 223.7179\n",
      "Epoch 800, Validation Loss: 209.5476\n",
      "Epoch 850, Validation Loss: 202.6453\n",
      "Epoch 900, Validation Loss: 200.2732\n",
      "Epoch 950, Validation Loss: 202.1256\n",
      "Epoch 1000, Validation Loss: 200.4334\n",
      "Epoch 1050, Validation Loss: 198.0477\n",
      "Epoch 1100, Validation Loss: 194.6512\n",
      "Epoch 1150, Validation Loss: 192.9426\n",
      "Epoch 1200, Validation Loss: 190.8740\n",
      "Epoch 1250, Validation Loss: 189.2503\n",
      "Epoch 1300, Validation Loss: 187.0080\n",
      "Epoch 1350, Validation Loss: 183.6585\n",
      "Epoch 1400, Validation Loss: 179.6353\n",
      "Epoch 1450, Validation Loss: 176.4086\n",
      "Epoch 1500, Validation Loss: 171.5395\n",
      "\n",
      "===== Run 4/10 =====\n",
      "Epoch 50, Validation Loss: 746.6791\n",
      "Epoch 100, Validation Loss: 733.0313\n",
      "Epoch 150, Validation Loss: 712.5486\n",
      "Epoch 200, Validation Loss: 685.4113\n",
      "Epoch 250, Validation Loss: 653.4381\n",
      "Epoch 300, Validation Loss: 617.7415\n",
      "Epoch 350, Validation Loss: 579.2304\n",
      "Epoch 400, Validation Loss: 536.4807\n",
      "Epoch 450, Validation Loss: 493.8532\n",
      "Epoch 500, Validation Loss: 446.7806\n",
      "Epoch 550, Validation Loss: 404.1892\n",
      "Epoch 600, Validation Loss: 365.4174\n",
      "Epoch 650, Validation Loss: 339.5660\n",
      "Epoch 700, Validation Loss: 326.8701\n",
      "Epoch 750, Validation Loss: 324.8204\n",
      "Epoch 800, Validation Loss: 323.1146\n",
      "Epoch 850, Validation Loss: 320.3458\n",
      "Epoch 900, Validation Loss: 313.6241\n",
      "Epoch 950, Validation Loss: 304.9976\n",
      "Epoch 1000, Validation Loss: 294.8583\n",
      "Epoch 1050, Validation Loss: 289.0506\n",
      "Epoch 1100, Validation Loss: 282.5490\n",
      "Epoch 1150, Validation Loss: 275.3397\n",
      "Epoch 1200, Validation Loss: 268.3535\n",
      "Epoch 1250, Validation Loss: 259.5865\n",
      "Epoch 1300, Validation Loss: 252.1192\n",
      "Epoch 1350, Validation Loss: 245.4868\n",
      "Epoch 1400, Validation Loss: 239.2868\n",
      "Epoch 1450, Validation Loss: 232.5667\n",
      "Epoch 1500, Validation Loss: 227.3967\n",
      "\n",
      "===== Run 5/10 =====\n",
      "Epoch 50, Validation Loss: 749.5816\n",
      "Epoch 100, Validation Loss: 739.3372\n",
      "Epoch 150, Validation Loss: 723.5497\n",
      "Epoch 200, Validation Loss: 703.9535\n",
      "Epoch 250, Validation Loss: 682.8615\n",
      "Epoch 300, Validation Loss: 658.1256\n",
      "Epoch 350, Validation Loss: 628.2462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400, Validation Loss: 589.9248\n",
      "Epoch 450, Validation Loss: 548.8285\n",
      "Epoch 500, Validation Loss: 508.2536\n",
      "Epoch 550, Validation Loss: 438.0364\n",
      "Epoch 600, Validation Loss: 358.0137\n",
      "Epoch 650, Validation Loss: 303.2509\n",
      "Epoch 700, Validation Loss: 270.6163\n",
      "Epoch 750, Validation Loss: 250.3989\n",
      "Epoch 800, Validation Loss: 238.6647\n",
      "Epoch 850, Validation Loss: 233.5156\n",
      "Epoch 900, Validation Loss: 231.6807\n",
      "Epoch 950, Validation Loss: 229.2785\n",
      "Epoch 1000, Validation Loss: 227.1199\n",
      "Epoch 1050, Validation Loss: 223.5247\n",
      "Epoch 1100, Validation Loss: 220.8455\n",
      "Epoch 1150, Validation Loss: 220.0146\n",
      "Epoch 1200, Validation Loss: 220.6351\n",
      "Epoch 1250, Validation Loss: 219.1726\n",
      "Epoch 1300, Validation Loss: 218.0353\n",
      "Epoch 1350, Validation Loss: 215.8617\n",
      "Epoch 1400, Validation Loss: 212.5839\n",
      "Epoch 1450, Validation Loss: 208.8293\n",
      "Epoch 1500, Validation Loss: 203.3826\n",
      "\n",
      "===== Run 6/10 =====\n",
      "Epoch 50, Validation Loss: 744.3665\n",
      "Epoch 100, Validation Loss: 734.1789\n",
      "Epoch 150, Validation Loss: 720.7626\n",
      "Epoch 200, Validation Loss: 704.5134\n",
      "Epoch 250, Validation Loss: 684.6524\n",
      "Epoch 300, Validation Loss: 659.9581\n",
      "Epoch 350, Validation Loss: 633.2034\n",
      "Epoch 400, Validation Loss: 605.9833\n",
      "Epoch 450, Validation Loss: 577.5418\n",
      "Epoch 500, Validation Loss: 542.2312\n",
      "Epoch 550, Validation Loss: 507.3979\n",
      "Epoch 600, Validation Loss: 462.9753\n",
      "Epoch 650, Validation Loss: 411.2443\n",
      "Epoch 700, Validation Loss: 335.7482\n",
      "Epoch 750, Validation Loss: 301.2093\n",
      "Epoch 800, Validation Loss: 279.7181\n",
      "Epoch 850, Validation Loss: 265.6368\n",
      "Epoch 900, Validation Loss: 257.7853\n",
      "Epoch 950, Validation Loss: 255.1456\n",
      "Epoch 1000, Validation Loss: 254.8964\n",
      "Epoch 1050, Validation Loss: 255.6931\n",
      "Epoch 1100, Validation Loss: 256.8639\n",
      "Epoch 1150, Validation Loss: 255.6757\n",
      "Epoch 1200, Validation Loss: 254.0934\n",
      "Epoch 1250, Validation Loss: 251.7110\n",
      "Epoch 1300, Validation Loss: 250.4625\n",
      "Epoch 1350, Validation Loss: 248.0085\n",
      "Epoch 1400, Validation Loss: 245.7580\n",
      "Epoch 1450, Validation Loss: 243.6818\n",
      "Epoch 1500, Validation Loss: 241.7205\n",
      "\n",
      "===== Run 7/10 =====\n",
      "Epoch 50, Validation Loss: 740.3482\n",
      "Epoch 100, Validation Loss: 733.3142\n",
      "Epoch 150, Validation Loss: 718.5762\n",
      "Epoch 200, Validation Loss: 700.3109\n",
      "Epoch 250, Validation Loss: 678.6792\n",
      "Epoch 300, Validation Loss: 650.6068\n",
      "Epoch 350, Validation Loss: 617.0851\n",
      "Epoch 400, Validation Loss: 573.8820\n",
      "Epoch 450, Validation Loss: 528.0212\n",
      "Epoch 500, Validation Loss: 485.8183\n",
      "Epoch 550, Validation Loss: 441.3899\n",
      "Epoch 600, Validation Loss: 378.4395\n",
      "Epoch 650, Validation Loss: 322.2039\n",
      "Epoch 700, Validation Loss: 281.1029\n",
      "Epoch 750, Validation Loss: 255.9296\n",
      "Epoch 800, Validation Loss: 244.3318\n",
      "Epoch 850, Validation Loss: 232.5336\n",
      "Epoch 900, Validation Loss: 229.4252\n",
      "Epoch 950, Validation Loss: 229.0683\n",
      "Epoch 1000, Validation Loss: 229.2780\n",
      "Epoch 1050, Validation Loss: 227.5818\n",
      "Epoch 1100, Validation Loss: 226.2267\n",
      "Epoch 1150, Validation Loss: 221.2224\n",
      "Epoch 1200, Validation Loss: 217.2450\n",
      "Epoch 1250, Validation Loss: 213.3323\n",
      "Epoch 1300, Validation Loss: 210.0183\n",
      "Epoch 1350, Validation Loss: 206.1564\n",
      "Epoch 1400, Validation Loss: 203.4931\n",
      "Epoch 1450, Validation Loss: 201.2262\n",
      "Epoch 1500, Validation Loss: 199.4879\n",
      "\n",
      "===== Run 8/10 =====\n",
      "Epoch 50, Validation Loss: 744.5705\n",
      "Epoch 100, Validation Loss: 735.3393\n",
      "Epoch 150, Validation Loss: 720.6768\n",
      "Epoch 200, Validation Loss: 704.8090\n",
      "Epoch 250, Validation Loss: 684.9283\n",
      "Epoch 300, Validation Loss: 662.3577\n",
      "Epoch 350, Validation Loss: 634.2638\n",
      "Epoch 400, Validation Loss: 598.9064\n",
      "Epoch 450, Validation Loss: 562.2018\n",
      "Epoch 500, Validation Loss: 527.5910\n",
      "Epoch 550, Validation Loss: 485.7709\n",
      "Epoch 600, Validation Loss: 414.7261\n",
      "Epoch 650, Validation Loss: 348.4309\n",
      "Epoch 700, Validation Loss: 295.8841\n",
      "Epoch 750, Validation Loss: 263.6792\n",
      "Epoch 800, Validation Loss: 240.7295\n",
      "Epoch 850, Validation Loss: 233.4274\n",
      "Epoch 900, Validation Loss: 226.9698\n",
      "Epoch 950, Validation Loss: 221.9267\n",
      "Epoch 1000, Validation Loss: 218.3109\n",
      "Epoch 1050, Validation Loss: 216.1315\n",
      "Epoch 1100, Validation Loss: 213.2556\n",
      "Epoch 1150, Validation Loss: 210.2189\n",
      "Epoch 1200, Validation Loss: 208.5809\n",
      "Epoch 1250, Validation Loss: 206.4124\n",
      "Epoch 1300, Validation Loss: 204.4878\n",
      "Epoch 1350, Validation Loss: 200.2920\n",
      "Epoch 1400, Validation Loss: 197.1471\n",
      "Epoch 1450, Validation Loss: 194.5540\n",
      "Epoch 1500, Validation Loss: 191.6362\n",
      "\n",
      "===== Run 9/10 =====\n",
      "Epoch 50, Validation Loss: 745.8665\n",
      "Epoch 100, Validation Loss: 736.1067\n",
      "Epoch 150, Validation Loss: 721.0752\n",
      "Epoch 200, Validation Loss: 703.8025\n",
      "Epoch 250, Validation Loss: 683.9962\n",
      "Epoch 300, Validation Loss: 660.9575\n",
      "Epoch 350, Validation Loss: 633.6021\n",
      "Epoch 400, Validation Loss: 598.0331\n",
      "Epoch 450, Validation Loss: 552.2267\n",
      "Epoch 500, Validation Loss: 503.7766\n",
      "Epoch 550, Validation Loss: 439.2841\n",
      "Epoch 600, Validation Loss: 363.6922\n",
      "Epoch 650, Validation Loss: 305.1879\n",
      "Epoch 700, Validation Loss: 269.0954\n",
      "Epoch 750, Validation Loss: 252.7738\n",
      "Epoch 800, Validation Loss: 244.8554\n",
      "Epoch 850, Validation Loss: 238.3137\n",
      "Epoch 900, Validation Loss: 232.5448\n",
      "Epoch 950, Validation Loss: 225.3794\n",
      "Epoch 1000, Validation Loss: 220.4197\n",
      "Epoch 1050, Validation Loss: 215.1408\n",
      "Epoch 1100, Validation Loss: 212.0548\n",
      "Epoch 1150, Validation Loss: 210.7559\n",
      "Epoch 1200, Validation Loss: 210.8411\n",
      "Epoch 1250, Validation Loss: 212.0504\n",
      "Epoch 1300, Validation Loss: 214.4764\n",
      "Epoch 1350, Validation Loss: 216.3176\n",
      "Epoch 1400, Validation Loss: 217.2183\n",
      "Epoch 1450, Validation Loss: 217.5227\n",
      "Epoch 1500, Validation Loss: 218.8893\n",
      "\n",
      "===== Run 10/10 =====\n",
      "Epoch 50, Validation Loss: 745.0833\n",
      "Epoch 100, Validation Loss: 736.3983\n",
      "Epoch 150, Validation Loss: 724.1194\n",
      "Epoch 200, Validation Loss: 708.9351\n",
      "Epoch 250, Validation Loss: 692.4347\n",
      "Epoch 300, Validation Loss: 673.2604\n",
      "Epoch 350, Validation Loss: 651.4437\n",
      "Epoch 400, Validation Loss: 618.6128\n",
      "Epoch 450, Validation Loss: 575.1678\n",
      "Epoch 500, Validation Loss: 527.7879\n",
      "Epoch 550, Validation Loss: 486.5251\n",
      "Epoch 600, Validation Loss: 450.0675\n",
      "Epoch 650, Validation Loss: 406.9258\n",
      "Epoch 700, Validation Loss: 361.3513\n",
      "Epoch 750, Validation Loss: 313.4794\n",
      "Epoch 800, Validation Loss: 275.3871\n",
      "Epoch 850, Validation Loss: 250.0618\n",
      "Epoch 900, Validation Loss: 234.7090\n",
      "Epoch 950, Validation Loss: 228.0606\n",
      "Epoch 1000, Validation Loss: 224.8364\n",
      "Epoch 1050, Validation Loss: 224.0644\n",
      "Epoch 1100, Validation Loss: 224.9346\n",
      "Epoch 1150, Validation Loss: 224.7766\n",
      "Epoch 1200, Validation Loss: 224.3211\n",
      "Epoch 1250, Validation Loss: 222.6412\n",
      "Epoch 1300, Validation Loss: 219.0780\n",
      "Epoch 1350, Validation Loss: 214.3998\n",
      "Epoch 1400, Validation Loss: 208.4491\n",
      "Epoch 1450, Validation Loss: 202.7627\n",
      "Epoch 1500, Validation Loss: 197.4633\n",
      "\n",
      "model2: 平均 RMSE = 10.555, 各次 RMSE = [12.238318, 10.982682, 11.930758, 10.428343, 11.04954, 12.59672, 11.019988, 9.810485, 10.420366, 11.845334]\n"
     ]
    }
   ],
   "source": [
    "scaler_temp = StandardScaler().fit(X2[[\"AnnealTemperature\"]])\n",
    "X2.loc[:, \"AnnealTemperature\"] = scaler_temp.transform(X2[[\"AnnealTemperature\"]])\n",
    "\n",
    "scaler_X1 = StandardScaler().fit(X1)\n",
    "scaler_X2 = StandardScaler().fit(X2)\n",
    "scaler_y = StandardScaler().fit(y)\n",
    "\n",
    "X1_scaled = X1\n",
    "X2_scaled = X2.values\n",
    "y_scaled = scaler_y.transform(y)\n",
    "\n",
    "joblib.dump(scaler_X1, \"scaler_X1.pkl\")\n",
    "joblib.dump(scaler_X2, \"scaler_X2.pkl\")\n",
    "joblib.dump(scaler_y, \"scaler_y.pkl\")\n",
    "\n",
    "def create_dataloaders(X, y, batch_size=64):\n",
    "    indices = np.arange(len(X))\n",
    "    dataset = TensorDataset(torch.tensor(X, dtype=torch.float32),\n",
    "                            torch.tensor(y, dtype=torch.float32),\n",
    "                            torch.tensor(indices, dtype=torch.long))\n",
    "    \n",
    "    n_total = len(dataset)\n",
    "    n_train = int(0.7 * n_total)\n",
    "    n_val = int(0.15 * n_total)\n",
    "    n_test = n_total - n_train - n_val\n",
    "    g = torch.Generator().manual_seed(SEED)\n",
    "    train_ds, val_ds, test_ds = random_split(dataset, [n_train, n_val, n_test], generator=g)\n",
    "    \n",
    "    return (DataLoader(train_ds, batch_size=batch_size, shuffle=False),\n",
    "            DataLoader(val_ds, batch_size=batch_size),\n",
    "            DataLoader(test_ds, batch_size=batch_size))\n",
    "\n",
    "train1, val1, test1 = create_dataloaders(X1_scaled, y)\n",
    "train2, val2, test2 = create_dataloaders(X2_scaled, y)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def train_model(model, train_dl, val_dl, epochs=1000, lr=1e-3):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb, _ in train_dl:\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, _ in val_dl:\n",
    "                val_loss += criterion(model(xb), yb).item()\n",
    "        val_loss /= len(val_dl)\n",
    "        \n",
    "\n",
    "        if (epoch+1) % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Validation Loss: {val_loss:.4f}\")\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "    return model, val_losses\n",
    "\n",
    "\n",
    "def evaluate_and_save_and_analyze(model, dl, X_original, scaler_y, filename_prefix):\n",
    "    model.eval()\n",
    "    preds, trues, orig_idx = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb, idx in dl:\n",
    "            pred = model(xb)\n",
    "            preds.append(pred.numpy())\n",
    "            trues.append(yb.numpy())\n",
    "            orig_idx.append(idx.numpy())\n",
    "    \n",
    "    preds = np.vstack(preds)\n",
    "    trues = np.vstack(trues)\n",
    "    orig_idx = np.hstack(orig_idx)\n",
    "    \n",
    "    rmse = np.sqrt(np.mean((preds - trues) ** 2))\n",
    "    \n",
    "    np.savetxt(f\"{filename_prefix}_pred_vs_true.txt\", np.hstack([trues, preds]),\n",
    "               header=\"true_value\\tpredicted_value\")\n",
    "    \n",
    "    df_results = X_original.iloc[orig_idx].copy()\n",
    "    df_results['True'] = trues\n",
    "    df_results['Pred'] = preds\n",
    "    df_results['Error'] = df_results['Pred'] - df_results['True']\n",
    "    \n",
    "    return rmse, df_results\n",
    "\n",
    "\n",
    "def train_and_evaluate_multiple_runs(model_class, input_dim, dataloaders, X_original, scaler_y, filename_prefix, \n",
    "                                     n_runs=10, epochs=1500, lr=1e-3):\n",
    "    all_preds, all_trues, all_indices = [], [], []\n",
    "    rmses = []\n",
    "    all_val_curves = []  \n",
    "\n",
    "    for run in range(n_runs):\n",
    "        print(f\"\\n===== Run {run+1}/{n_runs} =====\")\n",
    "        model = model_class(input_dim)\n",
    "        model, val_losses = train_model(model, dataloaders[0], dataloaders[1], epochs=epochs, lr=lr)\n",
    "\n",
    "        torch.save(model.state_dict(), f\"{filename_prefix}_run{run+1}.pt\")\n",
    "\n",
    "        rmse, df_results = evaluate_and_save_and_analyze(\n",
    "            model, dataloaders[2], X_original, scaler_y, f\"{filename_prefix}_run{run+1}\"\n",
    "        )\n",
    "        rmses.append(rmse)\n",
    "\n",
    "        all_preds.append(df_results['Pred'].values.reshape(-1, 1))\n",
    "        all_trues.append(df_results['True'].values.reshape(-1, 1))\n",
    "        all_indices.append(df_results.index.values)\n",
    "        all_val_curves.append(val_losses)\n",
    "\n",
    "    \n",
    "    all_indices = np.array(all_indices)\n",
    "    assert np.all(all_indices == all_indices[0]), \"Indices mismatch across runs!\"\n",
    "    \n",
    "    \n",
    "    preds_mean = np.mean(np.hstack(all_preds), axis=1).reshape(-1, 1)\n",
    "    trues_mean = all_trues[0]\n",
    "    rmse_mean = np.sqrt(np.mean((preds_mean - trues_mean) ** 2))\n",
    "\n",
    "    df_mean = X_original.iloc[all_indices[0]].copy()\n",
    "    df_mean[\"True\"] = trues_mean\n",
    "    df_mean[\"Pred_mean\"] = preds_mean\n",
    "    df_mean[\"Error_mean\"] = df_mean[\"Pred_mean\"] - df_mean[\"True\"]\n",
    "    df_mean.to_csv(f\"{filename_prefix}_mean_results.csv\", index=False)\n",
    "\n",
    "    \n",
    "    all_val_curves = np.array(all_val_curves)   # shape = (n_runs, epochs)\n",
    "    val_mean = np.mean(all_val_curves, axis=0)\n",
    "    val_std = np.std(all_val_curves, axis=0)\n",
    "\n",
    "    df_curve = pd.DataFrame({\n",
    "        \"epoch\": np.arange(1, epochs+1, 50),\n",
    "        \"val_loss_mean\": val_mean,\n",
    "        \"val_loss_std\": val_std\n",
    "    })\n",
    "    df_curve.to_csv(f\"{filename_prefix}_val_curve.csv\", index=False)\n",
    "\n",
    "    print(f\"\\n{filename_prefix}: 平均 RMSE = {rmse_mean:.3f}, 各次 RMSE = {rmses}\")\n",
    "    return rmse_mean, df_mean, df_curve\n",
    "\n",
    "\n",
    "\n",
    "# Model 1: composition only\n",
    "rmse1_mean, df1_mean, curve1 = train_and_evaluate_multiple_runs(\n",
    "    MLP, X1_scaled.shape[1], (train1, val1, test1), filtered_df_clean, scaler_y, \"model1\", n_runs=10\n",
    ")\n",
    "\n",
    "# Model 2: composition + AnnealTemperature\n",
    "rmse2_mean, df2_mean, curve2 = train_and_evaluate_multiple_runs(\n",
    "    MLP, X2_scaled.shape[1], (train2, val2, test2), filtered_df_clean, scaler_y, \"model2\", n_runs=10\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
